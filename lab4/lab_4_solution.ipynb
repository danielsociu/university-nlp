{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TASK\n",
    "\n",
    "### Deadline: 24 martie ora 23:59.\n",
    "### Formular pentru trimiterea temei: https://forms.gle/LKCMJmyhS8Z8m7Rc9\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Porniti de la un dataset artificial generat cu ajutorul metodei [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) din sklearn (minim 10000 de exemple, cu cel putin 100 de feature-uri relevante, grupate in cel putin 3 clase) sau puteti folosi un toy-dataset de [aici](https://scikit-learn.org/stable/datasets/toy_dataset.html).\n",
    "\n",
    "2. Definiti un model cu cel putin 3 layere lineare, dintre care ultimul are dimensiunea outputului egala cu numarul de clase. Aplicati dupa fiecare layer linear (cu exceptia ultimului) o functie de activare aleasa de voi.\n",
    "\n",
    "3. Impartiti datasetul in 80% train, 10% validare si 10% test. Creati cele trei dataloadere corespunzatoare fiecarui split.\n",
    "\n",
    "4. Definiti functia de loss (cross-entropy) si un optimizer (SGD, Adam, etc.).\n",
    "\n",
    "5. Antrenati modelul pentru mai multe epoci pe datele de train. La finalul fiecarei epoci evaluati performanta modelului pe datele de validare. Monitorizati la fiecare epoca eroarea medie si acuratetea pentru predictiile facute pe datele de train si separat pe datele de validare, pentru ca in cazul in care observati situatia de *overfit* sa puteti opri antrenarea.\n",
    "\n",
    "6. Salvati modelul cu cea mai buna eroare de validare, calculata la finalul epocii respective.\n",
    "\n",
    "7. Plotati pe acelasi grafic evolutia erorii de train si a erorii de validare la finalul fiecarei epoci. Plotati in alt grafic evolutia acuratetii pe datele de train si pe cele de validare.\n",
    "\n",
    "8. Evaluati modelul (eroare, acuratete, macro-f1, etc.) pe datele de test.\n",
    "\n",
    "9. Incercati sa adaugati dupa fiecare layer din model (cu exceptia ultimului) dropout cu un $p$ ales de voi. Analizati daca performanta unui model antrenat astfel este mai buna.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.32256635   9.56807414   5.5639312  ...  -5.25092757   3.3985659\n",
      "   -2.47976429]\n",
      " [ -0.81052623  11.33103421 -16.37653105 ...   6.25319125   0.91365487\n",
      "    2.69922974]\n",
      " [ -1.27729385  -3.13630879  -8.98495374 ...   8.60008904   3.98029703\n",
      "   10.75129489]\n",
      " ...\n",
      " [ -1.01534994   2.6587725   -4.03618551 ...   9.89684477  -0.16574878\n",
      "    3.81514782]\n",
      " [  0.85715604   2.05073202  -3.81690273 ...  -4.80180125   2.67341345\n",
      "   -3.32629954]\n",
      " [  0.60445274   8.42594061   5.27353489 ... -12.95242205  -0.61731374\n",
      "   -0.81125657]]\n",
      "[4 2 4 3 1 1 3 4 4 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# 1\n",
    "N_CLASSES = 5\n",
    "N_FEATURES = 200\n",
    "N_SAMPLES = 100000\n",
    "BATCH_SIZE = 128\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X, y = make_classification(n_samples=N_SAMPLES, n_classes=N_CLASSES, n_features=N_FEATURES, n_informative=150, random_state=42)\n",
    "print(X[:10])\n",
    "print(y[:10])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "Net(\n  (linear1): Linear(in_features=200, out_features=100, bias=True)\n  (linear2): Linear(in_features=100, out_features=40, bias=True)\n  (linear3): Linear(in_features=40, out_features=5, bias=True)\n  (activation): ReLU()\n)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(in_features=N_FEATURES, out_features=100)\n",
    "        self.linear2 = torch.nn.Linear(in_features=100, out_features=40)\n",
    "        self.linear3 = torch.nn.Linear(in_features=40, out_features=5)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.linear1(x))\n",
    "        x = self.activation(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "model1 = Net()\n",
    "model1.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# 3\n",
    "class RandomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (\n",
    "            self.X[item],\n",
    "            self.y[item]\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = RandomDataset(X_train, y_train)\n",
    "test_dataset = RandomDataset(X_test, y_test)\n",
    "val_dataset = RandomDataset(X_val, y_val)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# 4\n",
    "loss_fn1 = torch.nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
    "\n",
    "def test_fn(model: torch.nn.Module, loss_fn: torch.nn.Module, val_loader: DataLoader):\n",
    "    model.eval()\n",
    "\n",
    "    mean_loss = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for data, labels in val_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, labels)\n",
    "\n",
    "        mean_loss += loss.item()\n",
    "        true_labels.extend(labels.tolist())\n",
    "        predicted_labels.extend(output.max(1)[1].tolist())\n",
    "\n",
    "    mean_loss /= len(test_dataloader)\n",
    "\n",
    "    return mean_loss, true_labels, predicted_labels\n",
    "\n",
    "\n",
    "\n",
    "def train_fn(epochs: int, train_loader: DataLoader, val_loader: DataLoader,\n",
    "             model: torch.nn.Module, loss_fn: torch.nn.Module, optimizer: torch.optim):\n",
    "    best_loss = None\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        mean_loss, true_labels, predicted_labels = test_fn(model, loss_fn, val_loader)\n",
    "        accuracy = metrics.accuracy_score(true_labels, predicted_labels)\n",
    "        if best_loss is None or mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            torch.save(model.state_dict(), \"sol_model.pt\")\n",
    "        print (f'Epochs {e}')\n",
    "        print (f'Mean loss: {mean_loss}')\n",
    "        print (f'Accuracy:  {accuracy}', end='\\n\\n')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 0\n",
      "Mean loss: 0.20299707739790784\n",
      "Accuracy:  0.9337777777777778\n",
      "\n",
      "Epochs 1\n",
      "Mean loss: 0.1886285467992855\n",
      "Accuracy:  0.9457777777777778\n",
      "\n",
      "Epochs 2\n",
      "Mean loss: 0.14571391126211686\n",
      "Accuracy:  0.9624444444444444\n",
      "\n",
      "Epochs 3\n",
      "Mean loss: 0.1835971041759358\n",
      "Accuracy:  0.9525555555555556\n",
      "\n",
      "Epochs 4\n",
      "Mean loss: 0.1758631107550633\n",
      "Accuracy:  0.9608888888888889\n",
      "\n",
      "Epochs 5\n",
      "Mean loss: 0.1874343522270269\n",
      "Accuracy:  0.9574444444444444\n",
      "\n",
      "Epochs 6\n",
      "Mean loss: 0.17921605373768112\n",
      "Accuracy:  0.9564444444444444\n",
      "\n",
      "Epochs 7\n",
      "Mean loss: 0.2031447196591504\n",
      "Accuracy:  0.9586666666666667\n",
      "\n",
      "Epochs 8\n",
      "Mean loss: 0.169623201264988\n",
      "Accuracy:  0.9646666666666667\n",
      "\n",
      "Epochs 9\n",
      "Mean loss: 0.19352920089341416\n",
      "Accuracy:  0.9571111111111111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "train_fn(10, train_dataloader, val_dataloader, model1, loss_fn1, optimizer1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# 6\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}