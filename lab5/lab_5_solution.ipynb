{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TASK\n",
    "## Deadline: 31 martie ora 23:59.\n",
    "\n",
    "Formular pentru trimiterea temei: https://forms.gle/Bznaciv2MTy4kVL47\n",
    "\n",
    "Folosind intreg datasetul de mai sus (IMDb reviews) implementati urmatoarele cerinte:\n",
    "1. Impartiti setul de date in 80% train, 10% validare si 10% test\n",
    "2. Tokenizati textele si determinati vocabularul (in acest task vom lucra cu reprezentari la nivel de cuvant, NU la nivel de caracter); intrucat vocabularul poate fi foarte mare, incercati sa aplicati una dintre tehnicile mentionate in laborator (10K-20K de cuvinte ar fi o dimensiunea rezonabila a vocabularului)\n",
    "3. Transformati textele in vectori de aceeasi dimensiune folosind indexul vocabularului (alegeti o dimensiune maxima de circa 500-1000 de tokens)\n",
    "4. Implementati urmatoarea arhitectura:\n",
    "    * un Embedding layer pentru vocabularul determinat, ce contine vectori de dimensiune 100\n",
    "    * un layer dropout cu probabilitate 0.4\n",
    "    * un layer convolutional 1D cu 100 canale de input si 128 de canale de output, dimensiunea kernelului de 3 si padding 1; asupra rezultatului aplicati un layer de [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * asupra rezultatului ultimului layer, aplicati average-pooling 1D obtinand pentru fiecare canal media tuturor valorilor din vectorul sau corespunzator\n",
    "    * un layer feed-forward (linear) cu dimensiunea inputului 128, si 2 noduri pentru output (pentru clasificare in 0/1)\n",
    "5. Antrenati arhitectura folosind cross-entropy ca functie de loss si un optimizer la alegere. La finalul fiecarei epoci evaluati modelul pe datele de validare si salvati weighturile celui mai bun model astfel determinat\n",
    "6. Evaluati cel mai bun model obtinut pe datele de test.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alhiris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re\n",
    "from num2words import num2words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from unidecode import unidecode\n",
    "from pprint import pprint\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "('IMDB_Dataset.csv', <http.client.HTTPMessage at 0x7fbb382ecc10>)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "urlretrieve('https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv', 'IMDB_Dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              review  sentiment\n0  My family and I normally do not watch local mo...          1\n1  Believe it or not, this was at one time the wo...          0\n2  After some internet surfing, I found the \"Home...          0\n3  One of the most unheralded great works of anim...          1\n4  It was the Sixties, and anyone with long hair ...          0\n5  For my humanities quarter project for school, ...          1\n6  Arguebly Al Pacino's best role. He plays Tony ...          1\n7  Being a big fan of Stanley Kubrick's Clockwork...          1\n8  I reached the end of this and I was almost sho...          1\n9  There is no doubt that Halloween is by far one...          1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>My family and I normally do not watch local mo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Believe it or not, this was at one time the wo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>After some internet surfing, I found the \"Home...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>One of the most unheralded great works of anim...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>It was the Sixties, and anyone with long hair ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>For my humanities quarter project for school, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Arguebly Al Pacino's best role. He plays Tony ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Being a big fan of Stanley Kubrick's Clockwork...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>I reached the end of this and I was almost sho...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>There is no doubt that Halloween is by far one...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "data = pd.read_csv('IMDB_Dataset.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.20, random_state=42)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "data[:10]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [01:01<00:00, 651.18it/s]\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 671.87it/s]\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 655.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "def preprocess_review(review):\n",
    "    review_lower = review.lower()\n",
    "    review_numbers = re.sub(r\"((\\d+\\.)?\\d+)\", lambda x: num2words(x.group(0), lang=\"english\") , review_lower)\n",
    "    review_punctuation = review_numbers.translate(str.maketrans('', '', string.punctuation))\n",
    "    review_punctuation = re.sub(r\"\\s+\", ' ', review_punctuation)\n",
    "    return review_punctuation\n",
    "\n",
    "def tokenize_review(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    review_tokenized = word_tokenize(review)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    final_review = [lemmatizer.lemmatize(word) for word in review_tokenized if word not in stop_words]\n",
    "    return final_review\n",
    "\n",
    "def process_data(data):\n",
    "    final_data = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        review = data[i]\n",
    "        preprocessed_review = preprocess_review(review)\n",
    "        tokenized_review = tokenize_review(preprocessed_review)\n",
    "        final_data.append(tokenized_review)\n",
    "    return final_data\n",
    "\n",
    "x_train, y_train = process_data(train_df.review.tolist()), train_df.sentiment.tolist()\n",
    "x_test, y_test = process_data(test_df.review.tolist()), test_df.sentiment.tolist()\n",
    "x_val, y_val = process_data(val_df.review.tolist()), val_df.sentiment.tolist()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size in data: 147956\n",
      "['neelix', 'thiefturnedspy', 'subvert', 'eyea', 'influenced', 'barbershop', 'frommost', 'performingin', 'pengiun', 'witch']\n",
      "Vocabulary size in data: 17683\n",
      "['br', 'movie', 'film', 'one', 'like', 'time', 'good', 'character', 'even', 'get']\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(data):\n",
    "    units = set([unit for review in data for unit in review])\n",
    "    return units\n",
    "\n",
    "def word_frequency(data, min_apparitions):\n",
    "    all_words = [word for reviews in data for word in reviews]\n",
    "    sorted_vocab = sorted(dict(Counter(all_words)).items(), key=lambda pair: pair[1], reverse=True)\n",
    "    final_vocab = [k for k,v in sorted_vocab if v > min_apparitions]\n",
    "\n",
    "    return final_vocab\n",
    "\n",
    "total_words = get_vocab(x_train)\n",
    "print(f'Total vocabulary size in data: {len(total_words)}')\n",
    "print(list(total_words)[:10])\n",
    "\n",
    "vocabulary = word_frequency(x_train, min_apparitions=16)\n",
    "print(f'Vocabulary size in data: {len(vocabulary)}')\n",
    "print(vocabulary[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40000, 1000])\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "def vectorize_data(data, word_indices, one_hot=False):\n",
    "    vectorized = []\n",
    "    for sentence in data:\n",
    "        indexed_sentence = [word_indices[word] if word in word_indices.keys() else word_indices['UNK'] for word in sentence]\n",
    "\n",
    "        if one_hot:\n",
    "            indexed_sentence = np.eye(len(word_indices))[indexed_sentence]\n",
    "\n",
    "        vectorized.append(indexed_sentence)\n",
    "\n",
    "    return vectorized\n",
    "\n",
    "def make_padding(data, max_length=500):\n",
    "    return torch.tensor([\n",
    "        sentence[:max_length] + [1] * max(0, max_length - len(sentence))\n",
    "        for sentence in data\n",
    "    ])\n",
    "\n",
    "\n",
    "word_indices = dict((word, index + 2) for index, word in enumerate(vocabulary))\n",
    "indices_word = dict((index + 2, word) for index, word in enumerate(vocabulary))\n",
    "\n",
    "indices_word[0] = 'UNK'\n",
    "word_indices['UNK'] = 0\n",
    "\n",
    "indices_word[1] = 'PAD'\n",
    "word_indices['PAD'] = 1\n",
    "\n",
    "vocabulary_size = len(indices_word)\n",
    "\n",
    "x_train_vectorized = make_padding(vectorize_data(x_train, word_indices), max_length=1000)\n",
    "x_test_vectorized = make_padding(vectorize_data(x_test, word_indices), max_length=1000)\n",
    "x_val_vectorized = make_padding(vectorize_data(x_val, word_indices), max_length=1000)\n",
    "print(x_train_vectorized.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 4\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item], self.labels[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocabulary_size, 100, padding_idx=1)\n",
    "        self.dropout1 = torch.nn.Dropout(0.4)\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=100, out_channels=128, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm1d(num_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2)\n",
    "        ) # 1000 -> 500\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm1d(num_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2)\n",
    "        ) # 500 -> 250\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm1d(num_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2)\n",
    "        ) # 250 -> 125\n",
    "        self.average_layer = torch.nn.AvgPool1d(kernel_size=125) # 125 -> 1\n",
    "        self.convolutions = torch.nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.conv2,\n",
    "            self.conv3,\n",
    "            self.average_layer\n",
    "        )\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear = torch.nn.Linear(in_features=128, out_features=2)\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            self.flatten,\n",
    "            self.linear\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        x = self.convolutions(embeddings)\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = Model(vocabulary_size=vocabulary_size).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 5\n",
    "def validation_fn(net: torch.nn.Module, test_loader: DataLoader):\n",
    "    net.eval()\n",
    "    all_predictions = torch.tensor([])\n",
    "    all_targets = torch.tensor([])\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.long().to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = net(inputs)\n",
    "\n",
    "        predictions = output.argmax(1)\n",
    "        all_targets = torch.cat([all_targets, targets.detach().cpu()])\n",
    "        all_predictions = torch.cat([all_predictions, predictions.detach().cpu()])\n",
    "\n",
    "    val_acc = (all_predictions == all_targets).float().mean().numpy()\n",
    "    return val_acc\n",
    "\n",
    "\n",
    "\n",
    "def train_fn(epochs: int, train_loader: DataLoader, test_loader: DataLoader,\n",
    "             net: torch.nn.Module, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    best_val_acc = 0\n",
    "    for epoch_n in range(epochs):\n",
    "        print(f\"Epoch #{epoch_n + 1}\")\n",
    "        net.train()\n",
    "        with tqdm(train_loader, unit='batch') as t_loader:\n",
    "            for batch in t_loader:\n",
    "                net.zero_grad()\n",
    "\n",
    "                inputs, targets = batch\n",
    "                inputs = inputs.long().to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "\n",
    "                output = net(inputs)\n",
    "                loss = loss_fn(output, targets)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        val_acc = validation_fn(net, test_loader)\n",
    "        print(f'Epoch {epoch_n + 1} has accuracy {val_acc}')\n",
    "        if val_acc > best_val_acc:\n",
    "            torch.save(net.state_dict(), \"./model\")\n",
    "            best_val_acc = val_acc\n",
    "        # validare\n",
    "\n",
    "    print(\"Best validation accuracy \", best_val_acc)\n",
    "\n",
    "train_dataset = Dataset(x_train_vectorized, y_train)\n",
    "test_dataset = Dataset(x_test_vectorized, y_test)\n",
    "val_dataset = Dataset(x_val_vectorized, y_val)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.72batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 has accuracy 0.6776000261306763\n",
      "Epoch #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 has accuracy 0.8740000128746033\n",
      "Epoch #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 44.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 has accuracy 0.8808000087738037\n",
      "Epoch #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 44.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 has accuracy 0.8697999715805054\n",
      "Epoch #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 44.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 has accuracy 0.8745999932289124\n",
      "Epoch #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 has accuracy 0.8777999877929688\n",
      "Epoch #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.74batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 has accuracy 0.8557999730110168\n",
      "Epoch #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 has accuracy 0.8781999945640564\n",
      "Epoch #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 has accuracy 0.8614000082015991\n",
      "Epoch #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.72batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 has accuracy 0.8546000123023987\n",
      "Epoch #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 44.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 has accuracy 0.8772000074386597\n",
      "Epoch #12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 44.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 has accuracy 0.8791999816894531\n",
      "Epoch #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 has accuracy 0.871999979019165\n",
      "Epoch #14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 has accuracy 0.8772000074386597\n",
      "Epoch #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 has accuracy 0.8736000061035156\n",
      "Epoch #16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 44.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 has accuracy 0.8763999938964844\n",
      "Epoch #17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 has accuracy 0.8799999952316284\n",
      "Epoch #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 44.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 has accuracy 0.8754000067710876\n",
      "Epoch #19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 has accuracy 0.8521999716758728\n",
      "Epoch #20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 43.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 has accuracy 0.8758000135421753\n",
      "Best validation accuracy  0.8808\n"
     ]
    }
   ],
   "source": [
    "train_fn(\n",
    "    epochs=20,\n",
    "    train_loader=train_dataloader,\n",
    "    test_loader=test_dataloader,\n",
    "    net=model,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy obtained on test data: 0.8848000168800354\n"
     ]
    }
   ],
   "source": [
    "best_model = Model(vocabulary_size=vocabulary_size).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(\"./model\"))\n",
    "best_model.eval()\n",
    "\n",
    "# I used test instead of validation above, so I will just use val here\n",
    "acc = validation_fn(best_model, val_dataloader)\n",
    "print(f'Accuracy obtained on test data: {acc}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}