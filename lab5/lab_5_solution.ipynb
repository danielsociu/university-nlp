{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TASK\n",
    "## Deadline: 31 martie ora 23:59.\n",
    "\n",
    "Formular pentru trimiterea temei: https://forms.gle/Bznaciv2MTy4kVL47\n",
    "\n",
    "Folosind intreg datasetul de mai sus (IMDb reviews) implementati urmatoarele cerinte:\n",
    "1. Impartiti setul de date in 80% train, 10% validare si 10% test\n",
    "2. Tokenizati textele si determinati vocabularul (in acest task vom lucra cu reprezentari la nivel de cuvant, NU la nivel de caracter); intrucat vocabularul poate fi foarte mare, incercati sa aplicati una dintre tehnicile mentionate in laborator (10K-20K de cuvinte ar fi o dimensiunea rezonabila a vocabularului)\n",
    "3. Transformati textele in vectori de aceeasi dimensiune folosind indexul vocabularului (alegeti o dimensiune maxima de circa 500-1000 de tokens)\n",
    "4. Implementati urmatoarea arhitectura:\n",
    "    * un Embedding layer pentru vocabularul determinat, ce contine vectori de dimensiune 100\n",
    "    * un layer dropout cu probabilitate 0.4\n",
    "    * un layer convolutional 1D cu 100 canale de input si 128 de canale de output, dimensiunea kernelului de 3 si padding 1; asupra rezultatului aplicati un layer de [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * asupra rezultatului ultimului layer, aplicati average-pooling 1D obtinand pentru fiecare canal media tuturor valorilor din vectorul sau corespunzator\n",
    "    * un layer feed-forward (linear) cu dimensiunea inputului 128, si 2 noduri pentru output (pentru clasificare in 0/1)\n",
    "5. Antrenati arhitectura folosind cross-entropy ca functie de loss si un optimizer la alegere. La finalul fiecarei epoci evaluati modelul pe datele de validare si salvati weighturile celui mai bun model astfel determinat\n",
    "6. Evaluati cel mai bun model obtinut pe datele de test.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alhiris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re\n",
    "from num2words import num2words\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unidecode import unidecode\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "('IMDB_Dataset.csv', <http.client.HTTPMessage at 0x7f55d7cf6fa0>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "urlretrieve('https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv', 'IMDB_Dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                              review  sentiment\n0  My family and I normally do not watch local mo...          1\n1  Believe it or not, this was at one time the wo...          0\n2  After some internet surfing, I found the \"Home...          0\n3  One of the most unheralded great works of anim...          1\n4  It was the Sixties, and anyone with long hair ...          0\n5  For my humanities quarter project for school, ...          1\n6  Arguebly Al Pacino's best role. He plays Tony ...          1\n7  Being a big fan of Stanley Kubrick's Clockwork...          1\n8  I reached the end of this and I was almost sho...          1\n9  There is no doubt that Halloween is by far one...          1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>My family and I normally do not watch local mo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Believe it or not, this was at one time the wo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>After some internet surfing, I found the \"Home...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>One of the most unheralded great works of anim...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>It was the Sixties, and anyone with long hair ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>For my humanities quarter project for school, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Arguebly Al Pacino's best role. He plays Tony ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Being a big fan of Stanley Kubrick's Clockwork...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>I reached the end of this and I was almost sho...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>There is no doubt that Halloween is by far one...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "data = pd.read_csv('IMDB_Dataset.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.20, random_state=42)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "data[:10]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [01:01<00:00, 648.60it/s]\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 652.53it/s]\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 647.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "def preprocess_review(review):\n",
    "    review_lower = review.lower()\n",
    "    review_numbers = re.sub(r\"((\\d+\\.)?\\d+)\", lambda x: num2words(x.group(0), lang=\"english\") , review_lower)\n",
    "    review_punctuation = review_numbers.translate(str.maketrans('', '', string.punctuation))\n",
    "    review_punctuation = re.sub(r\"\\s+\", ' ', review_punctuation)\n",
    "    return review_punctuation\n",
    "\n",
    "def tokenize_review(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    review_tokenized = word_tokenize(review)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    final_review = [lemmatizer.lemmatize(word) for word in review_tokenized if word not in stop_words]\n",
    "    return final_review\n",
    "\n",
    "def process_data(data):\n",
    "    final_data = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        review = data[i]\n",
    "        preprocessed_review = preprocess_review(review)\n",
    "        tokenized_review = tokenize_review(preprocessed_review)\n",
    "        final_data.append(tokenized_review)\n",
    "    return final_data\n",
    "\n",
    "x_train, y_train = process_data(train_df.review.tolist()), train_df.sentiment.tolist()\n",
    "x_test, y_test = process_data(test_df.review.tolist()), test_df.sentiment.tolist()\n",
    "x_val, y_val = process_data(val_df.review.tolist()), val_df.sentiment.tolist()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size in data: 147956\n",
      "['lionsgate', 'hackes', 'pulpshould', 'hernandez', 'brommells', 'assetbr', 'smallvillebr', 'koppikar', 'earthtwo', 'mariska']\n",
      "Vocabulary size in data: 17683\n",
      "['br', 'movie', 'film', 'one', 'like', 'time', 'good', 'character', 'even', 'get']\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(data):\n",
    "    units = set([unit for review in data for unit in review])\n",
    "    return units\n",
    "\n",
    "def word_frequency(data, min_apparitions):\n",
    "    all_words = [word for reviews in data for word in reviews]\n",
    "    sorted_vocab = sorted(dict(Counter(all_words)).items(), key=lambda pair: pair[1], reverse=True)\n",
    "    final_vocab = [k for k,v in sorted_vocab if v > min_apparitions]\n",
    "\n",
    "    return final_vocab\n",
    "\n",
    "total_words = get_vocab(x_train)\n",
    "print(f'Total vocabulary size in data: {len(total_words)}')\n",
    "print(list(total_words)[:10])\n",
    "\n",
    "vocabulary = word_frequency(x_train, min_apparitions=16)\n",
    "print(f'Vocabulary size in data: {len(vocabulary)}')\n",
    "print(vocabulary[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}